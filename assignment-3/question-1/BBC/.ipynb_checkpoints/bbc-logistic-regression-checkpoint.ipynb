{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f8e3409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import *\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9886b183",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "26c12d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(['a', 'about', 'above', 'across', 'after', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'among', 'an', 'and', 'another', 'any', 'anybody', 'anyone', 'anything', 'anywhere', 'are', 'area', 'areas', 'around', 'as', 'ask', 'asked', 'asking', 'asks', 'at', 'away', 'b', 'back', 'backed', 'backing', 'backs', 'be', 'became', 'because', 'become', 'becomes', 'been', 'before', 'began', 'behind', 'being', 'beings', 'best', 'better', 'between', 'big', 'both', 'but', 'by', 'c', 'came', 'can', 'cannot', 'case', 'cases', 'certain', 'certainly', 'clear', 'clearly', 'come', 'could', 'd', 'did', 'differ', 'different', 'differently', 'do', 'does', 'done', 'down', 'down', 'downed', 'downing', 'downs', 'during', 'e', 'each', 'early', 'either', 'end', 'ended', 'ending', 'ends', 'enough', 'even', 'evenly', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'f', 'face', 'faces', 'fact', 'facts', 'far', 'felt', 'few', 'find', 'finds', 'first', 'for', 'four', 'from', 'full', 'fully', 'further', 'furthered', 'furthering', 'furthers', 'g', 'gave', 'general', 'generally', 'get', 'gets', 'give', 'given', 'gives', 'go', 'going', 'good', 'goods', 'got', 'great', 'greater', 'greatest', 'group', 'grouped', 'grouping', 'groups', 'h', 'had', 'has', 'have', 'having', 'he', 'her', 'here', 'herself', 'high', 'high', 'high', 'higher', 'highest', 'him', 'himself', 'his', 'how', 'however', 'i', 'if', 'important', 'in', 'interest', 'interested', 'interesting', 'interests', 'into', 'is', 'it', 'its', 'itself', 'j', 'just', 'k', 'keep', 'keeps', 'kind', 'knew', 'know', 'known', 'knows', 'l', 'large', 'largely', 'last', 'later', 'latest', 'least', 'less', 'let', 'lets', 'like', 'likely', 'long', 'longer', 'longest', 'm', 'made', 'make', 'making', 'man', 'many', 'may', 'me', 'member', 'members', 'men', 'might', 'more', 'most', 'mostly', 'mr', 'mrs', 'much', 'must', 'my', 'myself', 'n', 'necessary', 'need', 'needed', 'needing', 'needs', 'never', 'new', 'new', 'newer', 'newest', 'next', 'no', 'nobody', 'non', 'noone', 'not', 'nothing', 'now', 'nowhere', 'number', 'numbers', 'o', 'of', 'off', 'often', 'old', 'older', 'oldest', 'on', 'once', 'one', 'only', 'open', 'opened', 'opening', 'opens', 'or', 'order', 'ordered', 'ordering', 'orders', 'other', 'others', 'our', 'out', 'over', 'p', 'part', 'parted', 'parting', 'parts', 'per', 'perhaps', 'place', 'places', 'point', 'pointed', 'pointing', 'points', 'possible', 'present', 'presented', 'presenting', 'presents', 'problem', 'problems', 'put', 'puts', 'q', 'quite', 'r', 'rather', 'really', 'right', 'right', 'room', 'rooms', 's', 'said', 'same', 'saw', 'say', 'says', 'second', 'seconds', 'see', 'seem', 'seemed', 'seeming', 'seems', 'sees', 'several', 'shall', 'she', 'should', 'show', 'showed', 'showing', 'shows', 'side', 'sides', 'since', 'small', 'smaller', 'smallest', 'so', 'some', 'somebody', 'someone', 'something', 'somewhere', 'state', 'states', 'still', 'still', 'such', 'sure', 't', 'take', 'taken', 'than', 'that', 'the', 'their', 'them', 'then', 'there', 'therefore', 'these', 'they', 'thing', 'things', 'think', 'thinks', 'this', 'those', 'though', 'thought', 'thoughts', 'three', 'through', 'thus', 'to', 'today', 'together', 'too', 'took', 'toward', 'turn', 'turned', 'turning', 'turns', 'two', 'u', 'under', 'until', 'up', 'upon', 'us', 'use', 'used', 'uses', 'v', 'very', 'w', 'want', 'wanted', 'wanting', 'wants', 'was', 'way', 'ways', 'we', 'well', 'wells', 'went', 'were', 'what', 'when', 'where', 'whether', 'which', 'while', 'who', 'whole', 'whose', 'why', 'will', 'with', 'within', 'without', 'work', 'worked', 'working', 'works', 'would', 'x', 'y', 'year', 'years', 'yet', 'you', 'young', 'younger', 'youngest', 'your', 'yours', 'z', ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "0bfc6021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_letters(word):\n",
    "    return ''.join(filter(str.isalpha, word))\n",
    "\n",
    "def prepare(text):\n",
    "    words = text.split()\n",
    "    words = map(str.lower, words)\n",
    "    words = map(only_letters, words)\n",
    "    words = list(filter(lambda x: x not in stopwords, words))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "bc9ab93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "def load_dataset():\n",
    "    dataset = []\n",
    "    \n",
    "    for label in os.listdir('dataset'):\n",
    "        path = f'dataset/{label}'\n",
    "        \n",
    "        for filename in os.listdir(path):\n",
    "            with open(f'{path}/{filename}') as file:\n",
    "                text = file.read()\n",
    "                \n",
    "                words = prepare(text)\n",
    "                \n",
    "                dataset.append((words, label))\n",
    "    \n",
    "    return dataset\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "1366772c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "67d8dc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = os.listdir('dataset')\n",
    "categories\n",
    "\n",
    "categories_idx = {\n",
    "    'business':0, 'entertainment':1, 'politics':2, 'sport':3, 'tech':4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "4abd505c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_test_train(dataset, ratio=0.8):\n",
    "    counter = defaultdict(int)\n",
    "    \n",
    "    for words, label in dataset:\n",
    "        counter[label] += 1\n",
    "    \n",
    "    random.shuffle(dataset)\n",
    "    \n",
    "    x_train, y_train = [], []\n",
    "    x_test, y_test = [], []\n",
    "    \n",
    "    placed = defaultdict(int)\n",
    "    \n",
    "    for words, label in dataset:\n",
    "        if placed[label] < counter[label] * ratio:\n",
    "            x_train.append(words)\n",
    "            y_train.append(label)\n",
    "            placed[label] += 1\n",
    "        \n",
    "        else:\n",
    "            x_test.append(words)\n",
    "            y_test.append(label)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "745ddbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = split_test_train(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "8189f7a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1781, 444)"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train), len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "0958b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = set()\n",
    "\n",
    "\n",
    "for sample in x_train:\n",
    "    all_words.update(sample)\n",
    "    \n",
    "sorted_words = sorted(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "f0d757c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {}\n",
    "\n",
    "for idx, word in enumerate(sorted_words):\n",
    "    word_index[word] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "36a09848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28235"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "70ee2ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_document = len(x_train)\n",
    "\n",
    "word_document_freq = defaultdict(int)\n",
    "\n",
    "for doc in x_train:\n",
    "    for word in set(doc):\n",
    "        word_document_freq[word] += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "b86a9629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(sample):\n",
    "    # to sparse matrix\n",
    "    vec = defaultdict(int)\n",
    "    \n",
    "    for word in sample:\n",
    "        if word in word_index:\n",
    "            idx = word_index[word]\n",
    "            vec[idx] += 1    \n",
    "    \n",
    "    vec[len(word_index)] = 1\n",
    "    \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "26a41870",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vectorized = list(map(vectorize, x_train))\n",
    "y_train_encoded = list(map(lambda x: categories_idx[x], y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549bd1f1",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "1ac5b6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_scalar(x):\n",
    "    # to avoid overflow\n",
    "    if x < 0:\n",
    "        sigmoid = math.exp(x) / (1 + math.exp(x))\n",
    "    else:\n",
    "        sigmoid = 1 / (1 + math.exp(-x))\n",
    "\n",
    "    # to handle machine precision errors\n",
    "    sigmoid = max(0.0001, sigmoid)\n",
    "    sigmoid = min(0.9999, sigmoid)\n",
    "\n",
    "    return sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "2bca3fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(values):\n",
    "    max_value = float('-inf')\n",
    "    max_index = 0\n",
    "\n",
    "    for idx, value in enumerate(values):\n",
    "        if value > max_value:\n",
    "            max_index = idx\n",
    "            max_value = value\n",
    "\n",
    "    return max_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "ce183607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sample_x, weights):\n",
    "    # calculate z = sum(w * x + b)\n",
    "    # here bias (b) is also included in weights\n",
    "    z = 0\n",
    "    \n",
    "    for idx, tfidf in sample_x.items():\n",
    "        z += weights[idx] * tfidf\n",
    "\n",
    "    # sigmoid(z)\n",
    "    return sigmoid_scalar(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "37f713de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(predicted_value, actual_value):\n",
    "    y = actual_value\n",
    "    y_pred = predicted_value    \n",
    "\n",
    "    if y == 1:        \n",
    "        return -math.log(y_pred)\n",
    "\n",
    "    else:        \n",
    "        return -math.log(1 - y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "2c793784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_decent(X, label, weights=None, learning_rate=0.1):\n",
    "    #n_features = len(X[0])\n",
    "\n",
    "    # initialize weight with random values (equal length to x's features)\n",
    "    # works with sparse matrix\n",
    "    if weights == None:\n",
    "        weights = defaultdict(lambda:random.random())\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for x, y in zip(X, label):\n",
    "        y_pred = predict(x, weights)\n",
    "        \n",
    "        loss += cross_entropy_loss(y_pred, y)\n",
    "\n",
    "        # dw = (y_pred - y) * x\n",
    "        # weight = weight - learning_rate * dw\n",
    "        err = y_pred - y\n",
    "        for i, value in x.items():\n",
    "            dw_i = err * value\n",
    "            weights[i] -= learning_rate * dw_i\n",
    "\n",
    "    return weights, loss / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "e010f27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_binary_class(x_train, y_train, learning_rate, epoch, verbose=False):\n",
    "    weights = None\n",
    "\n",
    "    # for graphing\n",
    "    history = []\n",
    "\n",
    "    for i in range(epoch):\n",
    "        weights, loss = gradient_decent(x_train, y_train, weights, learning_rate)\n",
    "\n",
    "        history.append(loss)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{i}]\\n\\t- Cross entropy loss: {loss}\\n\")\n",
    "\n",
    "    return weights, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "a5743b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multiclass(x_train, y_train, learning_rate, epoch, verbose=False):\n",
    "    # identify unique classes\n",
    "    classes = range(0, len(categories))\n",
    "\n",
    "    # for each class create separate labels suitable for binary classification\n",
    "    labels = [[] for _ in classes]\n",
    "\n",
    "    for class_ in classes:\n",
    "        for label in y_train:              \n",
    "            if label == class_:\n",
    "                labels[class_].append(1)\n",
    "\n",
    "            else:\n",
    "                labels[class_].append(0)\n",
    "    \n",
    "    # now that we have separate labels for each class\n",
    "    # lets train binary classifier for each class\n",
    "    # (each classifier will identify whether sample x is member of class or not)\n",
    "\n",
    "\n",
    "    classifiers = [None] * len(classes)\n",
    "    histories = [None] * len(classes)\n",
    "\n",
    "    for cls_, label in enumerate(labels):\n",
    "        if verbose:\n",
    "            print(f'Training class [{categories[cls_]}]')\n",
    "\n",
    "        weights, history = train_binary_class(x_train, label, learning_rate, epoch)        \n",
    "        \n",
    "        classifiers[cls_] = weights\n",
    "        histories[cls_] = history\n",
    "\n",
    "        if verbose:\n",
    "            print('---------------------------------------------------------------------')\n",
    "    \n",
    "\n",
    "    return classifiers, histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "5837d4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(sample_x, model):\n",
    "    class_probabilities = []\n",
    "    \n",
    "    for weights in model:\n",
    "        class_probability = predict(sample_x, weights)        \n",
    "        class_probabilities.append(class_probability)\n",
    "        \n",
    "    \n",
    "    return argmax(class_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "9ca7ad1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum([predict_class(x_train_normalized[i], model) == y_train[i] for i in range(60000)])\n",
    "\n",
    "def confusion_matrix(test_x, label, model):\n",
    "    grid = [[0] * len(categories) for _ in range(len(categories))]\n",
    "    net_accuracy = 0        \n",
    "\n",
    "    for i in range(len(test_x)):\n",
    "        prediction = predict_class(test_x[i], model)                                 \n",
    "\n",
    "        grid[prediction][label[i]] += 1\n",
    "        \n",
    "#         print(prediction, label[i], categories[prediction])\n",
    "\n",
    "        if prediction == label[i]:\n",
    "            net_accuracy += 1              \n",
    "\n",
    "    return grid, net_accuracy / len(test_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25cfd8c",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0327049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0001\n",
      "Training class [business]\n",
      "---------------------------------------------------------------------\n",
      "Training class [entertainment]\n",
      "---------------------------------------------------------------------\n",
      "Training class [politics]\n",
      "---------------------------------------------------------------------\n",
      "Training class [sport]\n",
      "---------------------------------------------------------------------\n",
      "Training class [tech]\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.0001, 0.001, 0.01, 0.1, 1.0, 1.5]\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "models = []\n",
    "histories = []\n",
    "\n",
    "for eta in learning_rates:\n",
    "    print(f'Learning rate: {eta}')\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    model, history = train_multiclass(x_train_vectorized, y_train_encoded, eta, epochs, verbose=True)\n",
    "                \n",
    "    time_taken = time.perf_counter() - start_time  \n",
    "    \n",
    "    \n",
    "    print('Time taken:', time_taken)\n",
    "    \n",
    "    models.append(model)\n",
    "    histories.append(history)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d125a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_vectorized = list(map(vectorize, x_test))\n",
    "y_test_encoded = list(map(lambda x: categories_idx[x], y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cc5799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw confussion matrix\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=len(learning_rates))\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    \n",
    "    ax.set_label(learning_rates[i])\n",
    "    \n",
    "    cm, acc = confusion_matrix(x_test_vectorized, y_test, models[i])\n",
    "\n",
    "\n",
    "    ax.imshow(cm)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f201e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa543625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9582bf99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215c0b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
